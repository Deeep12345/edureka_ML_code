{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project -- Online News Popularity -- Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set: Online News Popularity Data Set \n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/online+news+popularity#\n",
    "<br> Note: Don't use the file from this URL. Rather use the dataset attached with project as column names are changed to remove spaces.\n",
    "\n",
    "\n",
    "**Attribute Information:<br>**\n",
    "Number of Attributes: 61 including target column -- shares\n",
    "\n",
    "Attribute Information: \n",
    "0. url: URL of the article \n",
    "1. timedelta: Days between the article publication and the dataset acquisition\n",
    "2. n_tokens_title: Number of words in the title \n",
    "3. n_tokens_content: Number of words in the content \n",
    "4. n_unique_tokens: Rate of unique words in the content \n",
    "5. n_non_stop_words: Rate of non-stop words in the content \n",
    "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content \n",
    "7. num_hrefs: Number of links \n",
    "8. num_self_hrefs: Number of links to other articles published by Mashable \n",
    "9. num_imgs: Number of images \n",
    "10. num_videos: Number of videos \n",
    "11. average_token_length: Average length of the words in the content \n",
    "12. num_keywords: Number of keywords in the metadata \n",
    "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? \n",
    "14. data_channel_is_entertainment: Is data channel 'Entertainment'? \n",
    "15. data_channel_is_bus: Is data channel 'Business'? \n",
    "16. data_channel_is_socmed: Is data channel 'Social Media'? \n",
    "17. data_channel_is_tech: Is data channel 'Tech'? \n",
    "18. data_channel_is_world: Is data channel 'World'? \n",
    "19. kw_min_min: Worst keyword (min. shares) \n",
    "20. kw_max_min: Worst keyword (max. shares) \n",
    "21. kw_avg_min: Worst keyword (avg. shares) \n",
    "22. kw_min_max: Best keyword (min. shares) \n",
    "23. kw_max_max: Best keyword (max. shares) \n",
    "24. kw_avg_max: Best keyword (avg. shares) \n",
    "25. kw_min_avg: Avg. keyword (min. shares) \n",
    "26. kw_max_avg: Avg. keyword (max. shares) \n",
    "27. kw_avg_avg: Avg. keyword (avg. shares) \n",
    "28. self_reference_min_shares: Min. shares of referenced articles in Mashable \n",
    "29. self_reference_max_shares: Max. shares of referenced articles in Mashable \n",
    "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable \n",
    "31. weekday_is_monday: Was the article published on a Monday? \n",
    "32. weekday_is_tuesday: Was the article published on a Tuesday? \n",
    "33. weekday_is_wednesday: Was the article published on a Wednesday? \n",
    "34. weekday_is_thursday: Was the article published on a Thursday? \n",
    "35. weekday_is_friday: Was the article published on a Friday? \n",
    "36. weekday_is_saturday: Was the article published on a Saturday? \n",
    "37. weekday_is_sunday: Was the article published on a Sunday? \n",
    "38. is_weekend: Was the article published on the weekend? \n",
    "39. LDA_00: Closeness to LDA topic 0 \n",
    "40. LDA_01: Closeness to LDA topic 1 \n",
    "41. LDA_02: Closeness to LDA topic 2 \n",
    "42. LDA_03: Closeness to LDA topic 3 \n",
    "43. LDA_04: Closeness to LDA topic 4 \n",
    "44. global_subjectivity: Text subjectivity \n",
    "45. global_sentiment_polarity: Text sentiment polarity \n",
    "46. global_rate_positive_words: Rate of positive words in the content \n",
    "47. global_rate_negative_words: Rate of negative words in the content \n",
    "48. rate_positive_words: Rate of positive words among non-neutral tokens \n",
    "49. rate_negative_words: Rate of negative words among non-neutral tokens \n",
    "50. avg_positive_polarity: Avg. polarity of positive words \n",
    "51. min_positive_polarity: Min. polarity of positive words \n",
    "52. max_positive_polarity: Max. polarity of positive words \n",
    "53. avg_negative_polarity: Avg. polarity of negative words \n",
    "54. min_negative_polarity: Min. polarity of negative words \n",
    "55. max_negative_polarity: Max. polarity of negative words \n",
    "56. title_subjectivity: Title subjectivity \n",
    "57. title_sentiment_polarity: Title polarity \n",
    "58. abs_title_subjectivity: Absolute subjectivity level \n",
    "59. abs_title_sentiment_polarity: Absolute polarity level \n",
    "60. shares: Number of shares (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This note book is divided in two parts\n",
    "#### Part 1 -- Explore, Understand the Data and If required perform Wrangling\n",
    "#### Part 2 -- Apply various modeling techniques and use Root Mean Square Error (RMSE) to evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "**Import the usual libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Suppress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of Part I --  Explore, Understand the Data and If required perform Wrangling\n",
    "#### Get the Data\n",
    "\n",
    "** Use pandas to read data as a dataframe called df.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"OnlineNewsPopularity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs   ...    \\\n",
       "0                  0.815385        4.0             2.0       1.0   ...     \n",
       "1                  0.791946        3.0             1.0       1.0   ...     \n",
       "2                  0.663866        3.0             1.0       1.0   ...     \n",
       "3                  0.665635        9.0             0.0       1.0   ...     \n",
       "4                  0.540890       19.0            19.0      20.0   ...     \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.100000                    0.7              -0.350000   \n",
       "1               0.033333                    0.7              -0.118750   \n",
       "2               0.100000                    1.0              -0.466667   \n",
       "3               0.136364                    0.8              -0.369697   \n",
       "4               0.033333                    1.0              -0.220192   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                 -0.600              -0.200000            0.500000   \n",
       "1                 -0.125              -0.100000            0.000000   \n",
       "2                 -0.800              -0.133333            0.000000   \n",
       "3                 -0.600              -0.166667            0.000000   \n",
       "4                 -0.500              -0.050000            0.454545   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 -0.187500                0.000000   \n",
       "1                  0.000000                0.500000   \n",
       "2                  0.000000                0.500000   \n",
       "3                  0.000000                0.500000   \n",
       "4                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.187500     593  \n",
       "1                      0.000000     711  \n",
       "2                      0.000000    1500  \n",
       "3                      0.000000    1200  \n",
       "4                      0.136364     505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  39644.000000    39644.000000      39644.000000     39644.000000   \n",
       "mean     354.530471       10.398749        546.514731         0.548216   \n",
       "std      214.163767        2.114037        471.107508         3.520708   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.470870   \n",
       "50%      339.000000       10.000000        409.000000         0.539226   \n",
       "75%      542.000000       12.000000        716.000000         0.608696   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      39644.000000              39644.000000  39644.000000   \n",
       "mean           0.996469                  0.689175     10.883690   \n",
       "std            5.231231                  3.264816     11.332017   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.625739      4.000000   \n",
       "50%            1.000000                  0.690476      8.000000   \n",
       "75%            1.000000                  0.754630     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos      ...        \\\n",
       "count    39644.000000  39644.000000  39644.000000      ...         \n",
       "mean         3.293638      4.544143      1.249874      ...         \n",
       "std          3.855141      8.309434      4.107855      ...         \n",
       "min          0.000000      0.000000      0.000000      ...         \n",
       "25%          1.000000      1.000000      0.000000      ...         \n",
       "50%          3.000000      1.000000      0.000000      ...         \n",
       "75%          4.000000      4.000000      1.000000      ...         \n",
       "max        116.000000    128.000000     91.000000      ...         \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.095446               0.756728              -0.259524   \n",
       "std                 0.071315               0.247786               0.127726   \n",
       "min                 0.000000               0.000000              -1.000000   \n",
       "25%                 0.050000               0.600000              -0.328383   \n",
       "50%                 0.100000               0.800000              -0.253333   \n",
       "75%                 0.100000               1.000000              -0.186905   \n",
       "max                 1.000000               1.000000               0.000000   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "count           39644.000000           39644.000000        39644.000000   \n",
       "mean               -0.521944              -0.107500            0.282353   \n",
       "std                 0.290290               0.095373            0.324247   \n",
       "min                -1.000000              -1.000000            0.000000   \n",
       "25%                -0.700000              -0.125000            0.000000   \n",
       "50%                -0.500000              -0.100000            0.150000   \n",
       "75%                -0.300000              -0.050000            0.500000   \n",
       "max                 0.000000               0.000000            1.000000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count              39644.000000            39644.000000   \n",
       "mean                   0.071425                0.341843   \n",
       "std                    0.265450                0.188791   \n",
       "min                   -1.000000                0.000000   \n",
       "25%                    0.000000                0.166667   \n",
       "50%                    0.000000                0.500000   \n",
       "75%                    0.150000                0.500000   \n",
       "max                    1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity         shares  \n",
       "count                  39644.000000   39644.000000  \n",
       "mean                       0.156064    3395.380184  \n",
       "std                        0.226294   11626.950749  \n",
       "min                        0.000000       1.000000  \n",
       "25%                        0.000000     946.000000  \n",
       "50%                        0.000000    1400.000000  \n",
       "75%                        0.250000    2800.000000  \n",
       "max                        1.000000  843300.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['url', 'timedelta', 'n_tokens_title', 'n_tokens_content',\n",
       "       'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens',\n",
       "       'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n",
       "       'average_token_length', 'num_keywords',\n",
       "       'data_channel_is_lifestyle', ' data_channel_is_entertainment',\n",
       "       'data_channel_is_bus', 'data_channel_is_socmed',\n",
       "       'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min',\n",
       "       'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max',\n",
       "       'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg',\n",
       "       'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday',\n",
       "       'weekday_is_tuesday', 'weekday_is_wednesday',\n",
       "       'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday',\n",
       "       'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02',\n",
       "       'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'shares'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                               False\n",
       "timedelta                         False\n",
       "n_tokens_title                    False\n",
       "n_tokens_content                  False\n",
       "n_unique_tokens                   False\n",
       "n_non_stop_words                  False\n",
       "n_non_stop_unique_tokens          False\n",
       "num_hrefs                         False\n",
       "num_self_hrefs                    False\n",
       "num_imgs                          False\n",
       "num_videos                        False\n",
       "average_token_length              False\n",
       "num_keywords                      False\n",
       "data_channel_is_lifestyle         False\n",
       " data_channel_is_entertainment    False\n",
       "data_channel_is_bus               False\n",
       "data_channel_is_socmed            False\n",
       "data_channel_is_tech              False\n",
       "data_channel_is_world             False\n",
       "kw_min_min                        False\n",
       "kw_max_min                        False\n",
       "kw_avg_min                        False\n",
       "kw_min_max                        False\n",
       "kw_max_max                        False\n",
       "kw_avg_max                        False\n",
       "kw_min_avg                        False\n",
       "kw_max_avg                        False\n",
       "kw_avg_avg                        False\n",
       "self_reference_min_shares         False\n",
       "self_reference_max_shares         False\n",
       "                                  ...  \n",
       "weekday_is_monday                 False\n",
       "weekday_is_tuesday                False\n",
       "weekday_is_wednesday              False\n",
       "weekday_is_thursday               False\n",
       "weekday_is_friday                 False\n",
       "weekday_is_saturday               False\n",
       "weekday_is_sunday                 False\n",
       "is_weekend                        False\n",
       "LDA_00                            False\n",
       "LDA_01                            False\n",
       "LDA_02                            False\n",
       "LDA_03                            False\n",
       "LDA_04                            False\n",
       "global_subjectivity               False\n",
       "global_sentiment_polarity         False\n",
       "global_rate_positive_words        False\n",
       "global_rate_negative_words        False\n",
       "rate_positive_words               False\n",
       "rate_negative_words               False\n",
       "avg_positive_polarity             False\n",
       "min_positive_polarity             False\n",
       "max_positive_polarity             False\n",
       "avg_negative_polarity             False\n",
       "min_negative_polarity             False\n",
       "max_negative_polarity             False\n",
       "title_subjectivity                False\n",
       "title_sentiment_polarity          False\n",
       "abs_title_subjectivity            False\n",
       "abs_title_sentiment_polarity      False\n",
       "shares                            False\n",
       "Length: 61, dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q1. Looking at the data above what are your first thoughts about quality of data and modeling? ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"url\",\"timedelta\"],axis = 1, inplace = True)\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of Features\n",
    "Lets find the correlation among features (very important for successfull modelling)\n",
    "\n",
    "We will plot correlation matrix using Plotly HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-571b94d1e70d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m data = [\n\u001b[1;32m----> 2\u001b[1;33m     go.Heatmap(\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= df.astype(float).corr().values,\n",
    "        x=df.columns.values,\n",
    "        y=df.columns.values,\n",
    "        colorscale='Viridis',\n",
    "        reversescale = False,\n",
    "        text = True ,\n",
    "        opacity = 1.0\n",
    "        \n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Correlation of features',\n",
    "    xaxis = dict(ticks='', nticks=36),\n",
    "    yaxis = dict(ticks='' ),\n",
    "    width = 900, height = 700,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q2. What inference can be drawn from correlation heatmap? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few features which are not coorelated hence good for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Of Part 2 -- Apply various modeling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common imports for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure output is same each time code is run\n",
    "random_state = 101     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df_data,test_size=0.3):\n",
    "    X = df_data.copy()\n",
    "    X.drop(\"shares\",axis=1, inplace=True,)\n",
    "    y = df_data[\"shares\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X,y,X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform Simple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_linear_regression(df_data,standscalar=False):\n",
    "    X,y,X_train, X_test, y_train, y_test = get_data(df_data)\n",
    "    if(standscalar):\n",
    "        print (\"Regression after applying StandardScaler\")\n",
    "        X_scaler = StandardScaler()\n",
    "        X_train = X_scaler.fit_transform(X_train)\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "        \n",
    "#         y_scaler = StandardScaler()\n",
    "# #         y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]\n",
    "# #         y_test = y_scaler.transform(y_test[:, None])[:, 0]\n",
    "#         y_train = y_scaler.fit_transform(y_train)\n",
    "#         y_test = y_scaler.transform(y_test)\n",
    "\n",
    "       \n",
    "     # End of If for StandardScaler  \n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    y_pred = lm.predict(X_test)\n",
    "    y_train_pred = lm.predict(X_train)\n",
    "    print (\"RMSE on Train Data is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))) )\n",
    "    print (\"RMSE on Test Data  is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_test, y_pred))) )\n",
    "    return X,y,X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Approach 1: Linear Regression ** <br>\n",
    "** Q3: What is the RMSE on Test Data for Linear Regression? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on Train Data is: 12381.92 :\n",
      "RMSE on Test Data  is: 11110.16 :\n",
      "Median Value of Shares: 1400.0\n"
     ]
    }
   ],
   "source": [
    "X,y,X_train, X_test, y_train, y_test = perf_linear_regression(df)\n",
    "print (\"Median Value of Shares:\", y.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q: 4.\tChallenge Why Linear Regression should be used here ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tLinear Regression is a good starting point in case we need to predict continuous variables .  On Speed vs. Accuracy balance � it scores on Speed.  RMSE value from Linear Regression is a good indication that it is not sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Approach 2:  Use Standard Scaler to scale the data ** <br>\n",
    "StandardScaler removes the mean and scales the data to unit variance. Standardization of a dataset is a common requirement for many machine learning estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling Regression with Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q5: Is the RMSE better than in Approach 1 (Linear Regression) in this case? If Not Why ? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression after applying StandardScaler\n",
      "RMSE on Train Data is: 12381.64 :\n",
      "RMSE on Test Data  is: 1913686794634.06 :\n"
     ]
    }
   ],
   "source": [
    "X,y,X_train, X_test, y_train, y_test = perf_linear_regression(df,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scalar behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). StandardScaler cannot guarantee balanced feature scales in the presence of outliers. Refer to http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "** So looks like Standard Scaling is bad choice **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do analysis of target value y and see if there are outliers and we can get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing shares count distribution data\n",
      "1100      2308\n",
      "1200      2018\n",
      "1300      1740\n",
      "1400      1592\n",
      "1500      1323\n",
      "1000      1177\n",
      "1600      1137\n",
      "1700      1013\n",
      "1800       889\n",
      "1900       810\n",
      "2000       774\n",
      "2100       661\n",
      "2200       631\n",
      "2300       511\n",
      "2400       506\n",
      "2600       442\n",
      "2500       441\n",
      "2700       435\n",
      "2800       359\n",
      "2900       356\n",
      "3000       348\n",
      "3100       306\n",
      "3200       282\n",
      "3400       271\n",
      "3300       259\n",
      "3500       252\n",
      "3800       240\n",
      "3600       233\n",
      "3700       223\n",
      "3900       200\n",
      "          ... \n",
      "38400        1\n",
      "139500       1\n",
      "168          1\n",
      "104          1\n",
      "53           1\n",
      "295          1\n",
      "72100        1\n",
      "92600        1\n",
      "309          1\n",
      "51500        1\n",
      "277          1\n",
      "43200        1\n",
      "39700        1\n",
      "149          1\n",
      "67700        1\n",
      "85           1\n",
      "59400        1\n",
      "8            1\n",
      "80800        1\n",
      "75600        1\n",
      "34500        1\n",
      "50700        1\n",
      "42400        1\n",
      "66900        1\n",
      "72900        1\n",
      "197600       1\n",
      "310800       1\n",
      "92           1\n",
      "57800        1\n",
      "82200        1\n",
      "Name: shares, Length: 1454, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "    print (\"\\nPrinting shares count distribution data\")\n",
    "    data = y.value_counts(ascending=False)\n",
    "    print (data)\n",
    "\n",
    "# #print (y.value_counts(ascending=False))\n",
    "\n",
    "# # print (\"\\nPrinting share count\")\n",
    "# # print (y.sort_values(ascending=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Approach 3: Detect Outlier and remove them from data set ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y,kde=False,color=\"green\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a48f871f2777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m data = [go.Bar(\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;31m# Use log of value_count to make graph more comprehendible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;31m#y= np.log2(y.value_counts().values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "data = [go.Bar(\n",
    "            x=y.value_counts().index.values,\n",
    "            # Use log of value_count to make graph more comprehendible \n",
    "            #y= np.log2(y.value_counts().values) \n",
    "            y= y.value_counts().values\n",
    "    )]\n",
    "\n",
    "py.iplot(data, filename='data-basic-bar',image_width=1200, image_height=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q6: Which data -- based on shares count should be dropped from dataset ?** <br>\n",
    "You can use just vizual cue here , we will have more formal approach in Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets get  a scatter plot of shares and respective counts\n",
    "# y_unique = y.unique()\n",
    "# sns.regplot(y_unique,y_unique,data=y,scatter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can try multiple values, but here I will drop the data below 100,000 and then below 20,000 shares **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generic function to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_threshold_data(df_copy,threshold,column_name):\n",
    "    df_adjusted = df_copy[df_copy[column_name] <= threshold]\n",
    "    print (\" Original Data Count:\", len(df_copy))\n",
    "    print (\" After Adjusting Data Count:\", len(df_adjusted))\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = filter_threshold_data(df,100000,\"shares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q7: What is RMSE for data filtered at 100,000 and 20,000? What inference can you draw?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_train, X_test, y_train, y_test = perf_linear_regression(df_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = filter_threshold_data(df,20000,\"shares\")\n",
    "X,y,X_train, X_test, y_train, y_test = perf_linear_regression(df_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tRMSE  for filter 100,000 � 5756.43, RMSE for filter 20,000 � 2670.63 \n",
    "Key Point is outliers skew the model hence it is important to drop them for dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Approach 4: Lets try improve the model by using DecisionTreeRegressor for regression ** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q8. Why we should use DecisionTree? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons for using Decision Tree\n",
    "1. Decision trees implicitly perform variable screening or feature selection\n",
    "2. Nonlinear relationships between parameters do not affect tree performance\n",
    "3. It is easy to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Decision Tree Regressor\n",
    "http://scikit-learn.org/stable/modules/tree.html#tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_regression(max_depth,X_train,y_train,X_test,y_test):\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print (\"Tree Max Depth is:\", max_depth)\n",
    "    print (\"RMSE on Train Data is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))) )\n",
    "    print (\"RMSE on Test Data is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_test, y_pred))) )\n",
    "    #print (model.decision_path)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q9. For what max_depth you get least RMSE for Test Data? ** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tRMSE -- 2698.59  on test data is least at max_depth = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q10. Why RMSE increases after certain max_depth? Is this sign of overfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very important to understand. Beyond a certain max_depth tree will start to overfit. Meaning higher no. of nodes mean more complicated model.  Sign of overfit is when training error is significantly less than test error.  In this case it starts happening from max_depth 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = filter_threshold_data(df,20000,\"shares\")\n",
    "X,y,X_train, X_test, y_train, y_test = get_data(df_adjusted)\n",
    "max_depth_count = len(X.columns)\n",
    "\n",
    "depth_range = [1,2,5,10,15,20,25,30,35,40,max_depth_count]\n",
    "print (\"Starting Decision Tree Regression:\")\n",
    "for depth in depth_range:\n",
    "    decision_tree_regression(depth,X_train,y_train,X_test,y_test)\n",
    "    \n",
    "# End of decision tree testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Approach 5: Combining multiple techniques ** <br>\n",
    "We will use the following\n",
    "1. Use MinMaxScaler to Scale the Data\n",
    "2. Use Principal Component Analysis to restrict the no. of dimensions\n",
    "3. Use Ensemble technique AdaBoost With DecisionTreeRegressor to further improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q11. Why MinMaxScaler should be used ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinMaxScaler is the most famous scaling algorithm, and follows the following formula for each feature:\n",
    "xi�min(x)/(max(x)�min(x) )\n",
    "\n",
    "It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n",
    "This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying MinMaxScaler\n",
    "\n",
    "** Q12. While applying min max scaling to normalize your features, do you apply min max scaling on the entire dataset before splitting it into training, validation and test data? -- My Favourite Question **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to understand this point \n",
    " Split it, then scale. Imagine it this way: you have no idea what real-world data looks like, so you couldn't scale the training data to it. Your test data is the surrogate for real-world data, so you should treat it the same way.  To reiterate: Split, scale your training data, then use the scaling from your training data on the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted = filter_threshold_data(df,20000,\"shares\")\n",
    "X,y,X_train, X_test, y_train, y_test = get_data(df_adjusted,)\n",
    "min_max_norm = MinMaxScaler()\n",
    "X_train_norm = min_max_norm.fit_transform(X_train)\n",
    "X_test_norm = min_max_norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA -- Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q13. What is the significance of using PCA here ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally do not want to feed a large number of features directly into a machine learning algorithm since some features may be irrelevant or the �intrinsic� dimensionality may be smaller than the number of features. PCA reduces dimension and make the model leaner and better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the PCA for variance upto 95 % <br>\n",
    "** Q14. How many features are there after variance is limited to 95% ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-92cfb644d338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train_normreduced\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train_normreduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_normreduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_norm)\n",
    "X_train_normreduced =  pd.DataFrame(pca.transform(X_train_norm))\n",
    "X_train_normreduced = X_train_normreduced.loc[:,pca.explained_variance_ratio_.cumsum()<0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train_normreduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got 22 features for around 95 % variance. Lets use this info to do modeling further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=22)\n",
    "df_adjusted = filter_threshold_data(df,20000,\"shares\")\n",
    "X,y,X_train, X_test, y_train, y_test = get_data(df_adjusted,)\n",
    "min_max_norm = MinMaxScaler()\n",
    "X_train_norm = min_max_norm.fit_transform(X_train)\n",
    "X_test_norm = min_max_norm.transform(X_test)\n",
    "\n",
    "X_train_norm  = pca.fit_transform(X_train_norm)\n",
    "X_test_norm = pca.transform(X_test_norm)\n",
    "X_train_norm =  pd.DataFrame(X_train_norm)\n",
    "X_test_norm = pd.DataFrame(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming y . We can choose different log base also such as 2, 10 etc\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q15. Why use AdaBoostRegressor ? ** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic function to use AdaBoostRegression for different dataset and estimator values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost_regression(X_train_norm,y_train_log,X_test_norm,y_test_log,n_estimators):\n",
    "    AdaDecision = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4, min_samples_leaf= 5, min_samples_split= 5),\n",
    "                                n_estimators=n_estimators)\n",
    "    AdaDecision.fit(X_train_norm, y_train_log)\n",
    "    y_pred = AdaDecision.predict(X_test_norm)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    \n",
    "    y_train_pred = pd.DataFrame(AdaDecision.predict(X_train_norm) )\n",
    "    y_train_orig_pred = y_train_pred.apply(lambda x: np.exp(x))\n",
    "    \n",
    "    y_orig_pred = y_pred.apply(lambda x: np.exp(x)) # Convert Output Back from Log to Original Value\n",
    "    print (\"Estimator Count in AdaBoostRegressor:\", n_estimators)\n",
    "    print (\"RMSE on Train Data is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_train, y_train_orig_pred))) )\n",
    "    print (\"RMSE on Test Data is: {:.2f} :\".format( np.sqrt(metrics.mean_squared_error(y_test, y_orig_pred))) )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q16. For which n_estimators values do you see lowest RMSE for Test Data ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the values in n_estimators_list and see what RMSE you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-3fa158d02cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_estimators_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_estimator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_estimators_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mada_boost_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_log\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_log\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "n_estimators_list = [10,20,30,40,50,100,125,150]\n",
    "for n_estimator in n_estimators_list:\n",
    "    ada_boost_regression(X_train_norm,y_train_log,X_test_norm,y_test_log,n_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q17. Compare the result above with DecisionTreeRegressor approach. What inference can you draw ? ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike DecisionTreeRegressor, AdaBoostRegressor does not overfit the data for higher number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q18. What is your final inference ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! to the real world of Machine Learning. We started with Linear Regression , used scaling , removed outliers, tried  DecisionTree and AdaBoost with PCA.   We improved RMSE on test data from 11100 to around 2700 with reduction in no. of features from 58 to 22.  No mean feat!.  \n",
    "Yet something is missing here. In spite of trying various modeling, our RMSE score did not improve much after filtering <= 20,000 shares records.\n",
    " Why ? Fundamental question could be is data correct and sufficient?\n",
    "Data is off course correct, but let's explore on sufficient part. \n",
    " It's very difficult to evaluate popularity of article based on just numerical features. Reason of popularity could also be the sentiments captured by article and the timing of publishing when certain event was occurring.  And these are some of the features which seem to be missing in data. \n",
    "Remember you will not always arrive at a optimal model and that is the point when you should start looking beyond the presented dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
